Bayes' theorem is a way to figure out conditional probability. It describes the probability of an event, based on prior knowledge of conditions that 
might be related to this event.

It can be mathematically stated as the formula:
P(A|B) = P(B|A) P (A) / (B)

a. P(A|B) called the posterior probability. It is the condition probability: the prob. of event A occuring given that event B is true;
b. P(B|A) is also a conditional probability: the probability of event B occurring given that A is true. It can also be interpreted as the likelihood 
of A given a fixed B because P(B|A)=L(A|B).
c. P(A) is the prioir probability
d. P(B) can be called the evidence, where P(B)<>0

THUS: poterior = Likelihood * prior / Evidence

The differences between 'Conditional probability' AND 'Bayes' Theorem'

Conditional Probability is the probability of an event A that is based on the occurence of another Event B.
P(A|B) = P(A UNION B) / P(B), where P(A UNION B) is the joint probability, the probility of Event A and B occurring together.

Bayes' theorem is derived using the definition of conditional probability. The Bayes Theorem formula includes two conditional probabilties.


# Example 1: 
We have two bags.
Bag 1: 7 red balls + 4 blue balls
Bag 2: 5 red balls + 9 blue balls
Draw a ball randomly and it turns out to be red. What is the probability this ball is from Bag 1?

ANSWER:
X: the ball is from bag 1, Y: the ball is from bag 2. P(A)=P(B) = 1/2
A: the ball is a red ball; B: the ball is a blue ball. 

We have P(A|X) = 7/11, P(B|X) 5/14
What we want is P(X|A)

## !!! NOTICE: P(A) = P(A|X) P(X) + P(A|B)P(B) 

according to the Bayes Theorem formula: P(X|A) = P(A|X) * P(X) / P(A) = (7/11 * 1/2) /(7/11*1/2 + 5/14*1/2)=0.64


# Example 2:
Let us say P(Fire) means how often there is fire, and P(Smoke) means how often we see smoke, then:
P(Fire|Smoke) means how often there is fire when we can see smoke
P(Smoke|Fire) means how often we can see smoke when there is fire
Now we have:
dangerous fires are rare (1%)
but smoke is fairly common (10%) due to barbecues,
and 90% of dangerous fires make smoke
We can then discover the probability of dangerous Fire when there is Smoke:

ANSWER:
P(fire|Smoke) = P(Smoke|Fire) P(fire) / P(Smoke) = 0.9 * 0.01/0.1= 0.09 = 9%

# Example 3:
You are planning a picnic today, but the morning is cloudy
Oh no! 50% of all rainy days start off cloudy!
But cloudy mornings are common (about 40% of days start cloudy)
And this is usually a dry month (only 3 of 30 days tend to be rainy, or 10%)
What is the chance of rain during the day?

Answer:
P(rain|cloud) = P(cloud|rain) * P(rain) / P(cloud) = 0.5 *0.1/0.4 = 1/8


### "A" With Three (or more) Cases
We just saw "A" with two cases (A and not A), which we took care of in the bottom line.

When "A" has 3 or more cases we include them all in the bottom line:

P(A1|B) =  
P(A1)P(B|A1) /
 P(A1)P(B|A1) + P(A2)P(B|A2) + P(A3)P(B|A3) + ...etc
 
 # Example: The Art Competition has entries from three painters: Pam, Pia and Pablo
Pam put in 15 paintings, 4% of her works have won First Prize.
Pia put in 5 paintings, 6% of her works have won First Prize.
Pablo put in 10 paintings, 3% of his works have won First Prize.

What is the chance that Pam will win First Prize?

Answer:
P(Pam|First) = P(First|Pam) * P(Pam) / P(First) 
intotal, there are 15+5+10=30 paintings, P(Pam) = 15/30=1/2
P(First)=P(first|pam)P(pam) + p(first|pia)P(pia) + p(first|pablo)P(pablo) = 0.04*1/2 + 0.06 * 1/6 + 0.03 * 1/3

Thus P(Pam|First) = 0.04*1/2 / (0.04*1/2 + 0.06 * 1/6 + 0.03 * 1/3)=0.5





---------------------------------------------------- Naive Bayes ------------------------------------------------------

Naive Bayes uses a similar method to predict the probability of different class based on various attributes. 
This algorithm is mostly used in text classification and with problems having multiple classes.


The Naive Bayes is a classification algorithm that is suitable for <binary> and <multiclass> classification


In Machine Learning
Naïve Bayes algorithm is a <supervised>learning algorithm, which is based on Bayes theorem and used for solving <classificatio> problems.

It is mainly used in text classification that includes a high-dimensional training dataset.
Naïve Bayes Classifier is one of the simple and most effective Classification algorithms which helps in building the fast machine learning models that can make quick predictions.
It is a probabilistic classifier, which means it predicts on the basis of the probability of an object.
Some popular examples of Naïve Bayes Algorithm are spam filtration, Sentimental analysis, and classifying articles.




