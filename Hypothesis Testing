Three main hypothesis testings: student t-test, F-test and Chi-squared test

https://isoconsultantkuwait.com/2020/10/26/hypothesis-testing/

---------------------------------------------------------------- p -value -------------------------------------
Assume we have two samples, A and B. And we have observed two different sample mean values for A and B. We want to know the probability of seeing
this difference due to random chances alone.
So, for example, if we have a p-value equals 0.01. This means we have 1% of seeing this difference due random chances alone.
The smaller p - value means we have stronger evidence to reject the null hypothesis, which assumes there is no difference between the mean of these two samples.

When the p-value falls below the chosen alpha value, then we say the result of the test is statistically significant.

Statistical significance is a term used by researchers to state that it is unlikely their observations could have occurred under the null hypothesis of a statistical test. 
Significance is usually denoted by a p-value, or probability value.

---------------------------------------------------------------- test statistic -------------------------------------
A test statistic is a number calculated by a statistical test. 
It describes how far your observed data is from the null hypothesis of no relationship between variables or no difference among sample groups.

A test statistic is a random variable that is calculated from sample data and used in a hypothesis test. 

You can use test statistics to determine whether to reject the null hypothesis. 
The test statistic compares your data with what is expected under the null hypothesis. The test statistic is used to calculate the p-value.




---------------------------------------------------------------- Quantative & Categorical -------------------------------------
Quantitative variables are any variables where the data represent amounts (e.g. height, weight, or age).
Categorical variables are any variables where the data represent groups. This includes rankings (e.g. finishing places in a race), classifications (e.g. brands of cereal), and binary outcomes (e.g. coin flips).

---------------------------------------------------------------- Discrete & Continuous -------------------------------------
Discrete variables represent counts (e.g. the number of objects in a collection).
Continuous variables represent measurable amounts (e.g. water volume or weight).

-------------------------------------------------------------- Type I and Type II errors -------------------------------------
In statistics, a Type I error is a false positive conclusion, while a Type II error is a false negative conclusion.
Type I error (false positive): the test result says you have coronavirus, but you actually don’t.
Type II error (false negative): the test result says you don’t have coronavirus, but you actually do.

Type I error: rejecting the null hypothesis when it’s actually true. 
The risk of committing this error is the significance level (alpha or α) you choose.

Type II error: 
not rejecting the null hypothesis when it’s actually false.
Power is the extent to which a test can correctly detect a real effect when there is one.
The risk of a Type II error is inversely related to the statistical power of a study.
The higher the statistical power, the lower the probability of making a Type II error.

How do you reduce the Type I error?
The risk of making a Type I error is the significance level (or alpha) that you choose.
To reduce the Type I error probability, you can set a lower significance level
How do you reduce the Type II error?
The risk of making a Type II error is inversely related to the statistical power of a test.
To reduce the risk of a Type II error, you can increase the sample size or the significance level to increase statistical power.


----------------------------------------------------------------Statistical Power ---------------------------------------------------------
statistical power is the probability that a test will correctly reject a false null hypothesis.

In statistics, power refers to the likelihood of a hypothesis test detecting a true effect if there is one. 
A statistically powerful test is more likely to reject a false negative (a Type II error).

If you don’t ensure enough power in your study, you may not be able to detect a statistically significant result
even when it has practical significance. Your study might not have the ability to answer your research question.

----------------------------------------------------------------Effect size in statistics ---------------------------------------------------------
In statistics, an effect size is a number measuring the strength of the relationship between two variables in a population, 

Effect size tells you how meaningful the relationship between variables or the difference between groups is. 
It indicates the practical significance of a research outcome.

A large effect size means that a research finding has practical significance, while a small effect size indicates limited practical applications.

----------------------------------------------------------------  Statistical assumptions  ---------------------------------------------------------
Statistical tests make some common assumptions about the data they are testing:
1. Independence of observations (a.k.a. no autocorrelation): 
The observations/variables you include in your test are not related (for example, multiple measurements of a single test subject are not independent, while measurements of multiple different test subjects are independent).
2. Homogeneity of variance: 
the variance within each group being compared is similar among all groups. If one group has much more variation than others, it will limit the test’s effectiveness.
3. Normality of data: 
the data follows a normal distribution (a.k.a. a bell curve). This assumption applies only to quantitative data.

the data are normally distributed
the groups that are being compared have similar variance
the data are independent

---------------------------------------------- Choosing a parametric test: regression, comparison, or correlation---------------------------------------------------------

Regression tests
Regression tests look for cause-and-effect relationships. They can be used to estimate the effect of one or more continuous variables on another variable.
Simple linear regression, multiple linear regression, logistic regression

Comparison tests
Comparison tests look for differences among group means. They can be used to test the effect of a categorical variable on the mean value of some other characteristic.
Paired t-test, independent t-test, ANOVA, MANOVA

Correlation tests
Correlation tests check whether variables are related without hypothesizing a cause-and-effect relationship.
Pearson's r test


----------------------------------------------------Choosing a nonparametric test----------------------------------------------
Chi-squared test of independentce, Whilconxon Rank-Sum test, Wilcoxon Signed-rank test



- - - - - - - - - - - - - - - - Hypotheses Tests for Comparing  Single Population

- - - - - Comparing Mean (Variance Known)

>>>> Z Test 
Z = (x_bar - miu_0) /sigma_{x_bar} =  (x_bar - miu_0) / ( sigma_{x}/ sqrt(n) )


>>>> Student’s t Test
t = (x_bar - miu_0) / ( s / sqrt(n) )

x_bar = sample mean, miu_0 = target value or population mean
s = sample standard deviation. n= number of test samples


- - - - - Comparing Standard Deviations/ Variance

>>>> Chi Square (χ2) Test

when variance is known: 
χ2 = (n-1) s2 / (sigma_x) ^ 2
sample variance is s2,  number of samples = n.  df=n-1
χ2 = sum ( O - E)^ 2 / E
O for observed, E for expected, where the expected value = row total * column total / grand value




- - - - - - - - - - - - - - - - Hypotheses Test for Comparing  two Population
Comparing Two Means (Variance Known)
Z Test 


Comparing Two Means (Variance Unknown but Equal)
Independent t-Test.

Comparing Two Means (Variance Unknown and Unequal)
Independent t-Test.


Comparing Two Means (Paired t-test)
t_{paired} = d-bar / ( sd / sqrt(n). )

d = difference between each pair of values
d-bar = observed mean difference
sd = standard deviation of d

Comparing Two Standard Deviations
F-Test

F = (S1^2 / sigma1^2) /   (S2^2 / sigma2^2)

----------------------------------------------------------------  t - test ------------------------------------------------------------------------------------------
https://www.scribbr.com/statistics/t-test/



T检验适用于数据服从正态分布、但方差未知的情况，通过比较不同数据的均值，研究两组数据是否存在差异。适用于小样本数据。

T检验怎么用
首先要明确检验的目的，是单样本T检验、配对样本T检验还是独立样本T检验。
进行正态性检验，独立样本还需要进行方差齐性检验
选择合适的检验方法进行检验

T检验注意事项
无论哪种T检验、都要求数据服从正态或者近似正态分布。正态性的检验方法有：正态图、正态性检验、P-P图/Q-Q图等。（https://zhuanlan.zhihu.com/p/49456086）
独立样本T检验，除了要满足正态性，还需要满足方差齐性的前提条件。在方差齐性的情况下才可以使用独立样本T检验，如果方差不齐性，则应采用校正T检

TAKE AWAY:
Frequent asked questions:
1. what is a t-test?
A t-test is a statistical test that compares the means of two samples. It is used in hypothesis testing, with a null hypothesis that the difference 
in group means is zero and an alternate hypothesis that the difference in group means is different from zero.

2. What does a t-test measure?
A t-test measures the difference in group means divided by the pooled standard error of the two group means.
In this way, it calculates a number (the t-value) illustrating the magnitude of the difference between the two group means being compared, 
and estimates the likelihood that this difference exists purely by chance (p-value).

3. which t-test should I use?
Your choice of t-test depends on whether you are studying one group or two groups, and whether you care about the direction of the difference in group means.
a. If you are studying one group, use a paired t-test to compare the group mean over time or after an intervention, or use a one-sample t-test to compare the group mean to a standard value. 
If you are studying two groups, use a two-sample t-test.
b. If you want to know only whether a difference exists, use a two-tailed test. 
If you want to know if one group mean is greater or less than the other, use a left-tailed or right-tailed one-tailed test.

4. What is the difference between one-sample t-test and a paired t-test
a. A one-sample t-test is used to compare a single population to a standard value (for example, to determine whether the average lifespan 
of a specific town is different from the country average).
b. A paired t-test is used to compare a single population before and after some experimental intervention 
or at two different points in time (for example, measuring student performance on a test before and after being taught the material).

5. Can I use a t-test to measure the differencs of more than two groups?
A t-test should not be used to measure differences among more than two groups, because the error structure for a t-test will underestimate 
the actual error when many groups are being compared.
If you want to compare the means of several groups at once, it’s best to use another statistical test such as ANOVA or a post-hoc test.


By performing a t-test, one can say whether the difference between the two means is statistically significant or by chance alone. 

types of t-test:
a. Two samples t-test (Student t-test)
It compares the means of two independent samples. It is also called an unpaired t-test or a two-sample t-test. 
It is used when the population mean or standard deviation is unknown.

b. Paired samples t-test
It compares the means of the same group at different time periods. In other words, the t-test is conducted on dependent samples.
A paired t-test is used when we are interested in the difference between two variables for the same subject.

c. One-sample t-test
It compares the group mean to a standard value. It can determine whether an unknown population mean is different from a specific value.


One-sample, two-sample, or paired t-test?
If the groups come from a single population (e.g. measuring before and after an experimental treatment), perform a paired t-test.
If the groups come from two different populations (e.g. two different species, or people from two separate cities), perform a two-sample t-test (a.k.a. independent t-test).
If there is one group being compared against a standard value (e.g. comparing the acidity of a liquid to a neutral pH of 7), perform a one-sample t-test.

One-tailed or two-tailed t-test?
If you only care whether the two populations are different from one another, perform a two-tailed t-test.
If you want to know whether one population mean is greater than or less than the other, perform a one-tailed t-test.

T-test formula:
The t-test estimates the true difference between two group means using 
the ratio of the difference in group means over the pooled standard error of both groups. 

The formula for the [two-sample t-test] (a.k.a. the Student’s t-test) is shown below.

t= ( mean(x1) -  mean(x2)) / sqrt( s1^2/n1^2  + s2^2/n2^2)

A larger t-value indicates a more significant difference between the groups.
You can compare your calculated t-value against the values in a critical value chart to determine whether your t-value
is greater than what would be expected by chance. 
If so, you can reject the null hypothesis and conclude that the two groups are in fact different.


#### A pooled standard deviation is simply a weighted average of standard deviations from two or more independent groups.
Pooled standard deviation = √ (n1-1)s12 +  (n2-1)s22 /  (n1+n2-2)
n1, n2: Sample size for group 1 and group 2, respectively.
s1, s2: Standard deviation for group 1 and group 2, respectively.

t-test in R:
t.test(Petal.Length ~ Species, data = flower.data)

Presenting the results of a t-test
When reporting your t-test results, the most important values to include are the t-value, the p-value, and the degrees of freedom for the test.
You can also include the summary statistics for the groups being compared, namely the mean and standard deviation.

Example:
The difference in petal length between iris species 1 (Mean = 1.46; SD = 0.206) and iris species 2 (Mean = 5.54; SD = 0.569) 
was significant (t (30) = -33.7190; p < 2.2e-16).


----------------------------------------------------------------  Chi-squared test --------------------------------------------------------------------------------------
卡方检验是一种分析类别数据差异性（独立性）的方法。是一种通过频数进行检验的方法。
used to compare observed results with expected results.
原假设：观察频数与期望频数没有差异，或者两个变量相互独立。 The null hypothesis for a chi-square independence test is that two categorical variables are independent in some population.

2.2 卡方检验有什么用
卡方检验对一列数据进行统计检验，分析单个类别变量实际观测的比例与期望的比例是否一致。
交叉表卡方研究两组类别变量的关系：如性别与看不看直播是否有关系。
配对卡方研究实验过程中，用不同方法检测同一批人，看两个方法的效果是否有显著差异。

2.3 卡方检验怎么用
确定卡方检验的类型
选用合适的方法进行卡方检验

2.4 卡方检验注意事项
需要随机样本数据
理论频数不能太小

Also called the goodness-of-fit test

Steps:
a. determine your null and alternative hypothese. H0: there is no association between the two variables
b. choose level of significance (alpha) --> area in the tail
c. Find the critical Value
d. Find test statistic
e. Draw your conclusion




----------------------------------------------------------------  F test --------------------------------------------------------------------------------------
F检验主要用于方差齐性检验、方差分析、线性回归方程整体的显著性检验。

3.1 方差齐性检验
目的：方差齐性是方差分析和一些均值比较 [公式] 检验的重要前提，利用 [公式] 检验进行方差齐性检验是最原始的，但对数据要求比较高。

要求：样本来自两个独立的、服从正态分布的总体。
3.2 什么是方差分析
方差分析就是对试验数据进行分析，检验方差相等的多个正态总体均值是否相等，进而判断各因素对试验指标的影响是否显著。其原理认为不同处理组的均值间的差别基本来源有两个：实验条件和随机误差。其思想为通过分析研究不同来源的变异对总变异的贡献大小，从而确定可控因素对研究结果影响力的大小。

3.3 方差分析的分类
单因素方差分析
适用于问卷数据和实验数据，实验中只有一个因素改变的样本。判断该因素对样本的影响是否显著。

双因素方差分析
适用于实验数据，实验中有两个因素改变的样本。

多因素方差分析
适用于实验数据，实验中有多个因素改变的样本。

3.4 方差分析怎么用
判断因变量数据类型
类别数据用卡方检验， T检验一般用来比较两个总体的均值是否相同，而单因素方差分析可用于比较多个总体的均值是否相同。

确定方差分析的类型
确定实验过程中有几种因素发生了改变，如只有一组，则选择单因素方差分析；如有两组，则选择双因素方差分析；如有多组，则选择多因素方差分析。

正态性检验
方差齐性检验
选择合适的方法进行检验
事后多重比较
单因素方差分析如果呈现出显著性，说明不同组别之间确实存在显著的差异，事后多重比较可以得出两组间显著差异的大小。

3.5 方差分析注意事项
方差分析用来分析定量数据的变化情况，可以比较2组或多组数据的差异。
方差分析要求样本满足正态分布
方差分析的前提是方差齐性

