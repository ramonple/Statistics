Random Variables and proability distribution

Variables can be classified into discrete variable and continuous variable.

1. Discrete
For a discrete random variable, x, the probability distribution is defined by a probability mass function, denoted by f(x). 
This function provides the probability for each value of the random variable. In the development of the probability function 
for a discrete random variable, two conditions must be satisfied: 
(1) f(x) must be nonnegative for each value of the random variable, and 
(2) the sum of the probabilities for each value of the random variable must equal one.

2. Continuous
In the continuous case, the counterpart of the probability mass function is the probability density function, also denoted by f(x). 
For a continuous random variable, the probability density function provides the height or value of the function at any particular value of x; 
it does not directly give the probability of the random variable taking on a specific value. 
However, the area under the graph of f(x) corresponding to some interval, obtained by computing the integral of f(x) over that interval, provides the probability that the variable will take on a value within that interval. A probability density function must satisfy two requirements: 
(1) f(x) must be nonnegative for each value of the random variable, and 
(2) the integral over all values of the random variable must equal one.

E(X)
The expected value, or mean, of a random variable—denoted by E(x) or μ—is a weighted average of the values the random variable may assume. 
discrete: E(x) = Σxf(x) 
continuous:  ∫xf(x)dx

Var(X)
The variance of a random variable, denoted by Var(x) or σ2, is a weighted average of the squared deviations from the mean. 
discrete: Var(x) = σ2 = Σ(x − μ)^2 f(x)
continuous:Var(x) = σ2 = ∫(x − μ)^2 f(x)dx


# discrete probability distribution
Binomial
Geometric
Possion

# Continuous proability distribution
Uniform 
Normal


# discrete
You use the binomial distribution to compute probabilities for a process where only one of two possible outcomes may occur on each trial. 
The geometric distribution is related to the binomial distribution; you use the geometric distribution to determine the probability that a specified number of trials will take place before the first success occurs. 
You can use the Poisson distribution to measure the probability that a given number of events will occur during a given time frame.

Binomial X~B(n,p) 
the probability of getting exactly k sucesses in n independent Bernouli trails:
f(k,n,p) = Pr(x=k) = {n k}p^k (1-p)^(n-k) = ( n!/k!(n-k)!)(1-p)^(n-k)
E(X) = np, Var(X)=np(1-p)

Geometric
If the probability of success on each trial is p, then the probability that the kth trial (out of k trials) is the first success is
Pr(x=k) = (1-p) ^(k-1) * p
E(X) = 1/p, Var(x)= (1-p) / p^2

Poisson
A discrete random variable X is said to have a Poisson distribution, with parameter lambda >0, if it has a probability mass function given by:
p(k,lambda) = lambda^k * e^(-lambda) / k !
k is the number of occurrences
e is the euler's number
! is the factorial funtion
E(X)= Var(X) = lambda

# Continuous

Uniform
The uniform distribution is useful because it represents variables that are evenly distributed over a given interval. 
f(x) = 1/(b-a) for a<= x <= b
     = 0       for x < a or x > b
cumulative distribution function:
F(x) = 0           for x < a
     = x-a / x-b   for a <= x <= b
     = 1.          for x > b
     
     
Moments [of a statistical distribution]
The shape of any distribution can be described by its various ‘moments’. The first four are:
1) The mean, the first moment, which indicates the central tendency of a distribution.
2) The second moment is the variance, which indicates the width or deviation.
3) The third moment is the skewness, which indicates any asymmetric ‘leaning’ to either left or right.
4) The fourth moment is the Kurtosis, which indicates the degree of central ‘peakedness’ or, equivalently, the ‘fatness’ of the outer tails.


Normal 

f(x) = 1 / sigma * sqrt(2*pi)    e ^ - (x-miu) ^2/ (2 * sigma ^ 2)
for normal distribution, we have mean=median=mode, symmetry about the center

68% of values are within 1 standard deviation of the mean
95% of values are within2 standard deviations of the mean
99.7% of values are within 3 standard deviations of the mean

# Example: 95% of students at school are between 1.1m and 1.7m tall.
Mean = (1.1+1.7) / 2 = 1.4
range = 4 * std, thus std = 0.15

standard normal distribution: sigma =1 ,miu =0 
f(x) = 1/sqrt(2* pi) exp(- x^2/2)
Standardized
In statistics, standardized variables are variables that have been standardized to have a mean of 0 and a standard deviation of 1.
The number of standard deviations from the mean is also called the "Standard Score", "sigma" or "z-score".
Standardizing makes it easier to compare scores, even if those scores were measured on different scales.

z = (x-mean)/std
So to convert a value to a Standard Score ("z-score"):


Weibull distribution

f(x,lambda,k) = k/lambda (x/lambda) ^ (k-1) for x>= 0
              = 0                           for x<0
k > 0 shape parameter, lambda > 0 scale parameter

F(x,k,lambda) = 1 - e ^ - ( (x/lambda)^ k)


Chi-squre distribution
A chi-square distribution is a continuous distribution with degrees of freedom. 
It is the sum of the squares of N normally distributed random variables. Because the normal distribution is continuous, 
so is its square, and so is the sum of multiple squares.
It is also used to test the goodness of fit of a distribution of data. 
Additionally, chi-square distribution is a special case of the gamma distribution.


Exponential distribution
is the probability distribution of the time between events in a Poisson point process.
It is a particular case of the gamma distribution:
The exponential distribution predicts the wait time until the *very first* event. 
The gamma distribution, on the other hand, predicts the wait time until the *k-th* event occurs.

f(x,lambda) = lambda * e^ (- lambda x).  x>=0
            =. 0                         x<0
lambda is the rate parameter    

F(x,lambda) = 1 - e^(-lambda x).    x>=0
            = 0                     x<0
E(x) = 1/lambda
Var(X) = 1/lambda^2
The moments E(X^n) = n! / lambda ^ n



##### Special case of distribution parametrization
A binomial distribution with parameters n = 1 and p is a Bernoulli distribution with parameter p.
A negative binomial distribution with parameters n = 1 and p is a geometric distribution with parameter p.
A gamma distribution with shape parameter α = 1 and rate parameter β is an exponential distribution with rate parameter β.
A gamma distribution with shape parameter α = v/2 and rate parameter β = 1/2 is a chi-squared distribution with ν degrees of freedom.
A chi-squared distribution with 2 degrees of freedom wer(k = 2) is an exponential distribution with a mean value of 2 (rate λ = 1/2 .)
A Weibull distribution with shape parameter k = 1 and rate parameter β is an exponential distribution with rate parameter β.


